1):
For the iteration, it based on how we store the data in matrix in array. For a matrix A[r,c] (r - row, c - column). The location in the array is r(r+1)/2 + c. So when we do B*C the resulting A is : A [r,c] = sum{ B[r,i]* C[i,c]}(i is from 1 to N). So A[r(r+1)/2 + c] += B[r(r+1)/2 + i] * C [i(i+1)/2 + c]. In the code, ii = r(r+1)/2, k = i, kk = i(i+1)/2, j = c. But we want to keep j<= i, since we just want to iterate the half of A[r,c], and k <= i, since for B, the data beyond i is 0. And for B*C, since we need to iterate B's column and C's row and B's column <j is 0 and C's row >i is 0. So we just want the j<k<i then we set (k = j;k<=i). So the starting indice is (i,0) which is (i(i+1)/2+0) and the ending indice is (i,i) since the rest is zero so we just go to (i,i) which is (i(i+1)/2+i). The stride is N if we iterate over column, which it is the matrix size. 
In the stats_papi.c we implement PAPI and it profile two events which are the Level 2 data cache misses and Level 2 data cache accesses. In that case we could know how many times we got cache hit and cache misses per each running. Then if we have higher cache hit, we would decrease the latency then we could make the code faster.
 
2):
For interchage i and j, it would become faster. Because when we iterate over i, we store A[ii+j] in distinct cache lines and we don't have false sharing if we write data to A[ii+j]. But if we iterate j, then we iterate sequentially in the array, in that case different threads would write to different A[ii+j] but in same cache line so we need to wait data race. In that case interchange i and j would give us faster runtime. 
3):
If we increase the number of threads from 3 to 10 the runtime will decrease dramatically.
When it is static and we add the chunk size from 2 to 4 it won't affect the runtime. But when we use static default setting which use chunk size as loop number/ number of threads which is round 187, then it would become slower since fetcth big chunk size would lead to bad spacial locality but if we set static chunk size to 2 then it would become far more faster which is close to dynamic . 
If we use guided defualt which use the chunk size as loop number/ threads number and reassign chunk size according to the rest data. Compareing to dynamic, default guided doesn't work well since normally the chunk size is decreasing but the stride in array also decreasing. So for big data stride we need to use small chunk size in that case we won't have false sharing. So guided doesn't work good here.
However, if we use static by the chunk size as 8. It would give us better performance since the cache line size is around 64byte and the double is 8 byte so each line could have 8 data. In that case chunk size 8 would give us the best spacial locality.
